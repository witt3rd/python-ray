{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray: Cloud Native Distributed Computing Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Core Links\n",
    "- [ray.io](https://www.ray.io/)\n",
    "- [docs.ray.io](https://docs.ray.io/)\n",
    "- [github/ray-project](github.com/ray-project/ray)\n",
    "\n",
    "##### Papers and Posts\n",
    "- [Cloud Programming Simplified: A Berkeley View on Serverless Computing](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf)\n",
    "- [Ray 1.x Architecture](https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview)\n",
    "- [Modern Parallel and Distributed Python: A Quick Tutorial on Ray](https://towardsdatascience.com/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8)\n",
    "- [Combine the development experience of a laptop with the scale of the cloud](https://gradientflow.com/combine-the-development-experience-of-a-laptop-with-the-scale-of-the-cloud/)\n",
    "\n",
    "##### Videos\n",
    "- [Ray Core Tutorial](https://youtu.be/_KOlq2C-568) ([source code](https://github.com/derwenai/ray_tutorial)) ([slides](papers/Ray_Core_Tutorial.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray is:\n",
    "\n",
    "1. A simple and flexible framework for distributed cloud computing\n",
    "    - Simple: simple annotation to make functions & classes distribute.\n",
    "    - Flexible: create new distributed function calls & instances.  No batching required.\n",
    "2. A cloud-provider independent compute launcher/autoscaler\n",
    "3. An ecosystem of distributed computation libraries build with #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it \"fit\" with ML?\n",
    "\n",
    "[3rd generation ML architecture](https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures): tackling the problem of production ML architecure by making it programmable.\n",
    "\n",
    "Moves the focus to libraries instead of worried about distributed computation and underlying clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simpler way to build 1st/2nd generation ML pipelines\n",
    "\n",
    "- Flexibility of a programming language to define pipelines\n",
    "- More easily allows for shared components (e.g., feature transformation during training vs real-time)\n",
    "- \"Out of the box\" support for distributed ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Core\n",
    "\n",
    "A [pattern language](https://patterns.eecs.berkeley.edu/) for distibuted systems as a library in Python, Java, and C++.\n",
    "\n",
    "- **task-parallel** - stateless, data independence\n",
    "- **remote objects** - key/value store\n",
    "- **actor pattern** - messages among classes, managing state\n",
    "- **parallel iterators** - lazy, infinite sequences\n",
    "- `multiprocessing.Pool` - drop-in replacement\n",
    "- `joblib` - e.g., _scikit-learn_ back-end\n",
    "- Dask, Modin, Mars, etc.\n",
    "\n",
    "Mix and match as needed, without tight coupling to framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "Ray makes use of:\n",
    "- [closures and decorators in Python](https://towardsdatascience.com/closures-and-decorators-in-python-2551abbc6eb6) ([PEP 318](https://www.python.org/dev/peps/pep-0318/))\n",
    "- [futures and promises in Python](http://dist-prog-book.com/chapter/2/futures.html#introduction)\n",
    "- [asyncio in Python](https://docs.python.org/3/library/asyncio-task.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node:192.168.50.185': 1.0,\n",
       " 'GPU': 1.0,\n",
       " 'memory': 2483038619.0,\n",
       " 'object_store_memory': 1241519308.0,\n",
       " 'CPU': 4.0,\n",
       " 'accelerator_type:GT': 1.0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Start Ray. If you're connecting to an existing cluster, you would use\n",
    "# ray.init(address=<cluster-address>) instead.\n",
    "ray.init(\n",
    "    num_cpus=4,\n",
    "    ignore_reinit_error=True,              # Don't print error messages if a Ray instance is already running. Attach to it\n",
    "    logging_level=logging.ERROR,           \n",
    ")\n",
    "ray.cluster_resources()                    # get the cluster resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern: task-parallel (remote functions)\n",
    "\n",
    "- add `@ray.remote` decorator on a regular Python function\n",
    "- properties: _data independence_, _stateless_\n",
    "- patterns: [Task Parallelism](https://patterns.eecs.berkeley.edu/?page_id=208), [Task Graph](https://patterns.eecs.berkeley.edu/?page_id=609)\n",
    "- [Ray Internals: A Peek at `ray.get` - Stephanie Wang, Anyscale](https://www.youtube.com/watch?v=a1kNnQu6vGw)\n",
    "- [Ray Internals: Object Management with the Ownership Model](https://youtu.be/1oSBxTayfJc) ([slides](https://speakerdeck.com/anyscale/ray-internals-object-management-with-the-ownership-model-stephanie-wang-and-yi-cheng-anyscale))\n",
    "- reference: [Patterns for Parallel Programming](https://www.goodreads.com/book/show/85053.Patterns_for_Parallel_Programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1\n",
    "\n",
    "Simple remote function (task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "# A regular Python function can be decorated with @ray.remote\n",
    "@ray.remote\n",
    "def f(x):\n",
    "    return x * x\n",
    "\n",
    "# The function can be invoked by using the `remote` method\n",
    "futures = [f.remote(i) for i in range(4)]\n",
    "\n",
    "# The values it returns can be collected using the `get` method.\n",
    "print(ray.get(futures))  # [0, 1, 4, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2\n",
    "\n",
    "Parallel task execution and blocking on futures (object refs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the for loop took 0.006 seconds.\n",
      "The results are: [0, 1, 2, 3]\n",
      "Executing the example took 3.225 seconds.\n"
     ]
    }
   ],
   "source": [
    "# A function simulating a more interesting computation that takes one second.\n",
    "@ray.remote\n",
    "def slow_function(i):\n",
    "    time.sleep(1)\n",
    "    return i\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "results = []\n",
    "for i in range(4):\n",
    "  results.append(slow_function.remote(i))\n",
    "\n",
    "duration = time.perf_counter() - start_time\n",
    "print('Executing the for loop took {:.3f} seconds.'.format(duration))\n",
    "\n",
    "results = ray.get(results)\n",
    "print('The results are:', results)\n",
    "\n",
    "assert results == [0, 1, 2, 3], 'Did you remember to call ray.get?'\n",
    "\n",
    "duration = time.perf_counter() - start_time\n",
    "print('Executing the example took {:.3f} seconds.'.format(duration))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Use the UI to view the task timeline and to verify that the four tasks were executed in parallel. You can do this as follows.\n",
    "\n",
    "1. Run the following cell to generate a JSON file containing the profiling data.\n",
    "2. Download the timeline file by right clicking on `task_parallel_ex_2.json` in the **Files** tab in the navigator to the left, right clicking, and selecting  **\"Download\"**.\n",
    "3. Enter **chrome://tracing** into the Chrome web browser, click on the **\"Load\"** button, and select the downloaded JSON file.\n",
    "\n",
    "To navigate within the timeline:\n",
    "- Move around by clicking and dragging.\n",
    "- Zoom in and out by holding **alt** on Windows or **option** on Mac and scrolling.\n",
    "\n",
    "**NOTE:** The timeline visualization will only work in **Chrome**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.timeline(filename=\"output/task_parallel_ex_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3\n",
    "\n",
    "Passing object refs between tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def my_function():\n",
    "    return 1\n",
    "\n",
    "@ray.remote\n",
    "def function_with_an_argument(value):\n",
    "    return value + 1\n",
    "\n",
    "obj_ref1 = my_function.remote()\n",
    "assert ray.get(obj_ref1) == 1\n",
    "\n",
    "# You can pass an object ref as an argument to another Ray remote function.\n",
    "obj_ref2 = function_with_an_argument.remote(obj_ref1)\n",
    "assert ray.get(obj_ref2) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern: distributed objects\n",
    "\n",
    "Remote objects:\n",
    "- distributed shared-memory object store\n",
    "- think: passing variables across the cluster\n",
    "- Ray will do object spilling to disk if needed\n",
    "\n",
    "In Ray, we can create and compute on objects. We refer to these objects as remote objects, and we use object refs to refer to them. Remote objects are stored in shared-memory object stores, and there is one object store per node in the cluster. In the cluster setting, we may not actually know which machine each object lives on.\n",
    "\n",
    "An object ref is essentially a unique ID that can be used to refer to a remote object. If you’re familiar with futures, our object refs are conceptually similar.\n",
    "\n",
    "Object refs can be created in multiple ways:\n",
    "- by remote function calls\n",
    "- by `ray.put()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1\n",
    "\n",
    "Simple `put()` / `get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1\n",
    "obj_ref = ray.put(y)\n",
    "assert ray.get(obj_ref) == y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2\n",
    "\n",
    "Parallel `put()` / `get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ray.get([ray.put(i) for i in range(3)])\n",
    "assert result == [0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "class Counter(object):\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "\n",
    "    def increment(self):\n",
    "        self.n += 1\n",
    "\n",
    "    def read(self):\n",
    "        return self.n\n",
    "\n",
    "\n",
    "counters = [Counter.remote() for i in range(4)]\n",
    "[c.increment.remote() for c in counters]\n",
    "futures = [c.read.remote() for c in counters]\n",
    "print(ray.get(futures))  # [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ray Head and Worker Nodes](images/ray_head_worker_nodes.jpg)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b8993322de9988bf4d1f2a5a315dbb9729831e2e9420b872ad242b038970356"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('ray-d0a2q5pH': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
